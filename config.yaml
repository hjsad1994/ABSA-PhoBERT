# PhoBERT ABSA Configuration
# ========================================================================
# Model: PhoBERT (vinai/phobert-base) for 3-class classification
# Training: Advanced settings with FP16, 8-bit optimizer, cosine scheduler
# Dataset: Sentence-aspect pairs format
# ========================================================================

paths:
  data_dir: "data"
  train_file: "data/train.csv"
  train_oversampled_file: "data/train_oversampled.csv"
  validation_file: "data/val.csv"
  test_file: "data/test.csv"
  output_dir: "checkpoints/phobert_finetuned"
  evaluation_report: "results/evaluation_report.txt"
  predictions_file: "results/test_predictions.csv"
  log_dir: "logs"

model:
  name: "vinai/phobert-base"
  num_labels: 3  # Negative (0), Neutral (1), Positive (2)
  max_length: 256
  dropout: 0.1
  hidden_size: 768

valid_aspects:
  - Battery
  - Camera
  - Performance
  - Display
  - Design
  - Packaging
  - Price
  - Shop_Service
  - Shipping
  - General
  - Others

sentiment_labels:
  Negative: 0
  Neutral: 1
  Positive: 2

data:
  # Columns in CSV
  text_column: "sentence"
  aspect_column: "aspect"
  label_column: "sentiment"
  
  # Oversampling configuration
  # If use_oversampled_file is true, loads from train_oversampled_file
  # If false, performs oversampling on-the-fly during training
  use_oversampled_file: true  # Use pre-generated oversampled file
  oversampling: true  # Legacy flag for backward compatibility
  oversampling_strategy: "per_aspect"  # oversample each aspect independently

training:
  # Batch size (optimized for RTX 4060 8GB VRAM)
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32  # 2x (no gradients in eval)
  gradient_accumulation_steps: 4   # Effective batch = 64

  # Alternative for higher memory (12GB+):
  # per_device_train_batch_size: 32
  # per_device_eval_batch_size: 64
  # gradient_accumulation_steps: 2
  
  # Optimizer settings
  optim: "adamw_bnb_8bit"  # 8-bit AdamW for memory efficiency
  learning_rate: 2.0e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0
  
  # Learning rate scheduler
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.06
  
  # Training duration
  num_train_epochs: 5
  
  # Mixed precision (RTX 4060 Ada Lovelace)
  fp16: true
  fp16_opt_level: "O2"
  fp16_full_eval: false
  tf32: true
  
  # DataLoader settings
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2
  dataloader_persistent_workers: true
  
  # Memory optimization
  gradient_checkpointing: false
  auto_find_batch_size: false
  group_by_length: false
  
  # Evaluation & checkpointing
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_f1"
  greater_is_better: true
  
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  
  # Logging
  logging_strategy: "steps"
  logging_steps: 50
  logging_first_step: true
  report_to: []
  
  # Reproducibility
  seed: 42
  data_seed: 42
  
  # Misc
  disable_tqdm: false
  prediction_loss_only: false
  remove_unused_columns: true
  label_names: ["labels"]
  include_inputs_for_metrics: false

  # Loss configuration
  use_focal_loss: true
  focal_alpha: 0.25
  focal_gamma: 2.0

general:
  seed: 42
  device: "auto"
  log_level: "info"
